{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Compare_two_websites_by_using_spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hujukee/nju-demo/blob/master/Compare_two_websites_by_using_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWQCrsONHv9i",
        "outputId": "ff3c24e3-473c-4206-8028-2ace269d6bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "#比较两个院校中文门户网站的特色（2019/10/1-2020/10/1）\n",
        "# 数据抓取时间，美国中部时间2020/10/21 10：00pm\n",
        "# 研究问题详述：\n",
        "# 1、学校每月哪些新闻最受师生关注？\n",
        "# 2、两个学校每月最受关注的新闻有何异同？\n",
        "# 3、学校哪些部门发布月平均发表新闻最多？\n",
        "# 4、学校哪些部门的新闻热度最高？\n",
        "# 5、学校新闻更新频率？\n",
        "\n",
        "# 解决思路：\n",
        "# 1、通过月平均浏览频次，计算新闻的的热度。抽取热点新闻的命名实体（时间、地点、人物、组织名....）\n",
        "# 2、绘制树图？旭日图？\n",
        "\n",
        "\n",
        "import re\n",
        "# import nltk\n",
        "import spacy\n",
        "# import scispacy\n",
        "import pandas as pd\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nlp = spacy.load('zh_core_web_sm') # spaCy的语言模型\n",
        "# nlp_sciapacy = spacy.load(\"en_core_sci_lg\") # ScispaCy的语言模型\n",
        "spacy_stopwords = spacy.lang.zh.stop_words.STOP_WORDS #加载spacy的停用词\n",
        "\n",
        "# Show stopwords in NLTK\n",
        "# words = stopwords.words('english')\n",
        "# words[:10]\n",
        "\n",
        "# Add new stopwords\n",
        "# list_stopwords=[]\n",
        "# with open('/content/drive/My Drive/iConference/stopwords.txt','r') as f:\n",
        "#   ####################################################################################################\n",
        "#     for line in f.readlines():\n",
        "#         list_stopwords.append(line.strip())     \n",
        "# # List3.lower()\n",
        "# list_stopwords_lower = [s.lower() for s in list_stopwords]\n",
        "# words.extend(list_stopwords_lower)#  words 为全部停用词，type(words)  #list \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #词形还原\n",
        "# def stemmer2(word):\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     return lemmatizer.lemmatize(word)\n",
        "\n",
        "#剔除list中的重复项，不用list(set(list))\n",
        "def del_same(list):\n",
        "    list_x =[]\n",
        "    for i in list:\n",
        "        if i in list_x:\n",
        "            continue\n",
        "        else:\n",
        "            list_x.append(i)\n",
        "    return list_x\n",
        "\n",
        "#利用grammar，自动抽取短语    \n",
        "# def noun_chunk(abstract):\n",
        "#     # chunk adjectives and noun、JJR比较级的形容词、JJS最高级的形容词、VBN现在分词\n",
        "#     grammar = r\"\"\"\n",
        "#         NP: {<NN.*>+}   \n",
        "#     \"\"\"\n",
        "#     cp = nltk.RegexpParser(grammar)\n",
        "#     #NLTK词性标注\n",
        "#     # sent_token = nltk.word_tokenize(sentence)\n",
        "#     # tagging1 = nltk.pos_tag(sent_token) \n",
        "    \n",
        "#     #spaCy词性标注\n",
        "#     doc = nlp(abstract)\n",
        "#     tagging = [(token.text, token.tag_) for token in doc]\n",
        "#     #提取所需要的名词短语\n",
        "#     #对leaf[0]进行词形还原\n",
        "#     noun_phrases = [' '.join(stemmer2(leaf[0]) for leaf in tree.leaves())  #leaf[0]表示word，leaf[1]表示词性\n",
        "#                       for tree in cp.parse(tagging).subtrees() \n",
        "#                       if tree.label()=='NP']\n",
        "#     #剔除命名实体（人名、地名、机构名），去停用词\n",
        "#     noun_ner = [i.text for i in doc.ents if i.label_.lower() in [\"person\",\"org\",\"gpe\",\"loc\",\"date\",\"time\",\"precent\"]] #提取摘要中的命名实体\n",
        "#     noun_phrases_clean = [j for j in noun_phrases if j not in noun_ner and j not in spacy_stopwords] #将noun_phrases中的NER和停用词剔除\n",
        "#     result = del_same(noun_phrases_clean)\n",
        "#     return result #剔除重复出现的phrases\n",
        "\n",
        "#直接利用Scispacy抽取标题+摘要中的实体（仅仅是名词嘛？有待考证）\n",
        "# def noun_chunk(abstract):\n",
        "#   doc = nlp_sciapacy(abstract)\n",
        "#   entity_list_stem = []\n",
        "#   entity_list = list(doc.ents) #获取文本中的实体\n",
        "\n",
        "#   #spaCy词性标注\n",
        "#   doc = nlp_spacy(abstract)\n",
        "#   tagging = dict([(token.text, token.tag_) for token in doc])\n",
        "#   # print(tagging)\n",
        "\n",
        "#   for i in entity_list: #词形还原+判断 单个词构成的短语 的词性，若不为NN或NNS则排除\n",
        "#     string = ''\n",
        "#     list_split = str(i).split()\n",
        "#     if len(list_split) > 1:\n",
        "#       for j in list_split:\n",
        "#         string = string + ' ' + stemmer2(j)\n",
        "#       entity_list_stem.append(string.strip())\n",
        "#     elif len(list_split) == 1:\n",
        "#       if tagging.get(list_split[0]) == 'NN' or tagging.get(list_split[0]) == 'NNS':\n",
        "#         entity_list_stem.append(stemmer2(list_split[0].strip()))\n",
        "#       else:\n",
        "#         continue\n",
        "#     else:\n",
        "#       continue\n",
        "\n",
        "  # for i in entity_list: #词形还原\n",
        "  #   string = ''\n",
        "  #   list_split = str(i).split()\n",
        "  #   for j in list_split:\n",
        "  #     string = string + ' ' + stemmer2(j)\n",
        "  #   entity_list_stem.append(string)\n",
        "\n",
        "  # noun_phrases_clean = [j for j in entity_list_stem if j not in words] #剔除noun_phrases中的停用词、POS为非名词的\n",
        "  # result = del_same(noun_phrases_clean) #去重\n",
        "  # return result #剔除重复出现的phrases\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'zh_core_web_sm' (0.1.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.671 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-26209cb19d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zh_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# spaCy的语言模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# nlp_sciapacy = spacy.load(\"en_core_sci_lg\") # ScispaCy的语言模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mspacy_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOP_WORDS\u001b[0m \u001b[0;31m#加载spacy的停用词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zh_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, disable)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m             )\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    276\u001b[0m         serializers = OrderedDict(\n\u001b[1;32m    277\u001b[0m             (\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m\"pkuseg_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_pkuseg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m\"pkuseg_processors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_pkuseg_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/srsly/_json_api.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/srsly/util.py\u001b[0m in \u001b[0;36mforce_path\u001b[0;34m(location, require_exists)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_exists\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't read file: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't read file: /usr/local/lib/python3.6/dist-packages/zh_core_web_sm/zh_core_web_sm-0.1.0/tokenizer/cfg"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibG08jjotPEg",
        "outputId": "b691d9e6-a2e1-45f3-819e-28323810c8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(stemmer2('families'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "family\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBcFaRb7Hv9l"
      },
      "source": [
        "#定义适用于Dataframe的名词短语抽取函数\n",
        "def noun_phrase_extractor(dataframe,attribute):\n",
        "    series = dataframe[attribute]\n",
        "    return noun_chunk(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQmakCikHv9m"
      },
      "source": [
        "#调用主函数\n",
        "if __name__ == '__main__':\n",
        "    file = '/content/drive/My Drive/iConference/BIBM/AI_TARGET_TOTAL_COPY.xlsx'\n",
        "    ####################################################################################################\n",
        "    df = pd.read_excel(file,sheet_name='Sheet1')\n",
        "    df['title_abstract'] = df['title'] + '. ' + df['abstract'] #合并title和abstract\n",
        "\n",
        "    # df.drop(columns=['Unnamed: 3'],inplace = True) #删除未名列\n",
        "    # df.dropna(axis=0,how='any',inplace = True)\n",
        "    \n",
        "    #数据清洗和预处理（去掉括号，特殊字符）\n",
        "    # converting all letters to lower case\n",
        "    df['title_abstract'] = df['title_abstract'].str.lower() #字符串全部转化为小写\n",
        "    # #removing the punctuations like()[]{} and the text between them\n",
        "    df['title_abstract'] = df['title_abstract'].replace(r'\\(.*?\\)|\\\\.*', '', inplace=False, regex=True)\n",
        "    # #removing other punctuations, accent marks and other diacritics\n",
        "    df['title_abstract'] = df['title_abstract'].replace(r'\\”|\\#|\\%|\\&|\\’|\\(|\\)|\\*|\\+|\\,|\\/|\\:|\\<|\\=|\\>|\\?|\\@|\\[|\\]|\\^|\\_|\\`|\\{|\\}|\\~|\\$', '', inplace=False, regex=True)\n",
        "\n",
        "    df['noun_phrases']=df[df.notnull()].apply(noun_phrase_extractor,axis = 1, attribute = 'title_abstract')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LinNWQDbJixX"
      },
      "source": [
        "df.to_excel('/content/drive/My Drive/iConference/BIBM/BIBM_TOTAL_COPY.xls') #写入excel\n",
        "####################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAvwdnY0PAfh"
      },
      "source": [
        "#设置默认的文件路径\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwGlApq0IwKl"
      },
      "source": [
        "#下载WordNet，stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY-9IV3O9qLq"
      },
      "source": [
        "!pip install -q pandas\n",
        "!pip install -q jieba\n",
        "!pip install -q tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTZ7bJk1AMe7"
      },
      "source": [
        "#安装scispacy\n",
        "# !pip install spacy\n",
        "!pip3 install -U spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Redfo4mjzHdZ"
      },
      "source": [
        "#安装中文版spacy zh_core_sci_lg. 安装后需要重置页面，点击Runtime---Restart Runtime\n",
        "!pip install https://github.com/howl-anderson/Chinese_models_for_SpaCy/releases/download/v2.2.X-0.1.0/zh_core_web_sm-0.1.0.tar.gz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRDcfMKI0FL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S62bgAz7Hv9s"
      },
      "source": [
        "df[df.isnull().T.any()] #isnull()能够判断数据中元素是否为空值；T为转置；any()判断该行是否有空值。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqF_MxJRHv9v"
      },
      "source": [
        "#将词list转化为共现矩阵，并且转化为网络\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import re\n",
        "\n",
        "\n",
        "#词频统计（计数）\n",
        "def Counter(list):\n",
        "    dict={} #定义字典存放词及其频次\n",
        "    for i in list:\n",
        "        if i in dict.keys():\n",
        "            dict[i]+=1\n",
        "        else:\n",
        "            dict[i]=1\n",
        "    dict_sorted = {k: v for k, v in sorted(dict.items(), key=lambda item: item[1],reverse = True)} # 注意：这里按照频次降序排列\n",
        "    return dict_sorted\n",
        "\n",
        "#将 lst = [\"['NER', 'ML']\",\"['ML', 'DL', 'NB']\"]（元素为string）转化为 lst = [['NER', 'ML'],['ML', 'DL', 'NB']](元素为list)\n",
        "# def Convert_(list): #将列表内元素string转化为list\n",
        "#     d2_list = []\n",
        "#     for k in list:\n",
        "#         result_list = re.findall('[a-zA-Z0-9]+',k) #提取英文字母或数字\n",
        "#         d2_list.append(result_list)\n",
        "#     return d2_list\n",
        "\n",
        "#将Dataframe的一列 df['noun_phrases'] 转化为 二维列表(形如：list)\n",
        "# lst = [\n",
        "#     ['NER', 'ML'],\n",
        "#     ['ML', 'DL', 'NB'],\n",
        "#     ['NER', 'SVM', 'ML','NB'],\n",
        "#     ['ML', 'NB'],\n",
        "#     ['NER', 'DL', 'ML','NB']\n",
        "# ]\n",
        "\n",
        "\n",
        "#文献年份分段标准\n",
        "#1952-1956 AI birth\n",
        "#1957-1974 AI gold years\n",
        "#1975-1980 Ai winter\n",
        "#1981-1987 AI boom\n",
        "#1988-1993 Second AI winter\n",
        "#1994-2011 AI recovery?\n",
        "#2012-2019 AI property? 要不要按照每年细分？\n",
        "\n",
        "#对AAAI 划分时间\n",
        "df_1981_1987 = df[(df['year'] >= 1981) & (df['year'] <= 1987)]\n",
        "df_1988_1993 = df[(df['year'] >= 1988) & (df['year'] <= 1993)]\n",
        "df_1994_2011 = df[(df['year'] >= 1994) & (df['year'] <= 2011)]\n",
        "df_2012_2019 = df[(df['year'] >= 2012) & (df['year'] <= 2019)]\n",
        "####################################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EethXX0aJ8Z7",
        "outputId": "f53f4d7e-bf6b-4f52-ddaa-0f9e4cdab86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df_1994_2011)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "496"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ZZ8DEo2DjY"
      },
      "source": [
        "lst = df_2012_2019['noun_phrases'].values.tolist() #二维列表，存储每行的phrases，格式为[[p1,p2,p3],[p2,p3]...[p1,p3]]\n",
        "####################################################################################################\n",
        "length = len(lst) #文章的篇数\n",
        "lst1 = [i for item in lst for i in item]#二维列表转一维列表\n",
        "dict_all_phrases = Counter(lst1) #存储所有短语的字典，按照词频降序排列\n",
        "\n",
        "#把高频词输出\n",
        "df_high_freq_phrases = pd.DataFrame(dict_all_phrases,index=[0]).T #字典转化为Dataframe，设置key值为Index\n",
        "df_high_freq_phrases.to_csv('/content/drive/My Drive/iConference/BIBM/BIBM_high_freq_phrases_2012_2019.csv')\n",
        "####################################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWf7guTFEpl9",
        "outputId": "d427b3bd-8ff2-4304-c764-7d83c9af0461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "608"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6SKpnaPwzZn"
      },
      "source": [
        "dict_all_phrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTDGneEc45qr"
      },
      "source": [
        "dict_all_phrases #不重复短语的个数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvfVm34V4lBy"
      },
      "source": [
        "#统计出现次数为1的词的个数(高频词的确定，未采用Donohue J C 1973年的文献的方法)\n",
        "# def Numbers(**dict): #统计字典的值等于自定义value的健个数\n",
        "# dic_T = {} #值为{v1:[k1,k2],v2:[k3,k4],....}\n",
        "# dic_T1 = {}\n",
        "# for k, v in dict_all_phrases.items():\n",
        "#     dic_T.setdefault(v, []).append(k)\n",
        "#   # print(dic_T)\n",
        "# for k, v in dic_T.items():\n",
        "#     dic_T1[k] = len(v)\n",
        "# num_1 = dic_T1.get(1) #值为1的健(短语)的个数(也就是只出现一次的短语的个数)\n",
        "#   # print(dic_T1)\n",
        "#   # if 1 in dic_T1.keys():\n",
        "#   #   return dic_T1.get(1)\n",
        "#   # else:\n",
        "#   #   return 0\n",
        "\n",
        "# # num_1 = Numbers(**dict_all_phrases) #统计出现次数为1的词的个数\n",
        "# threshold = 0.5 * (np.sqrt(1 + 8 * num_1) - 1) #高频短语的界定（未采用Donohue J C 1973年的文献的方法，该文献针对的是单词，非短语）\n",
        "\n",
        "# 提取词频>threshold的词为高频词\n",
        "dict_threshold = {key: value for key, value in dict_all_phrases.items() if value > 2} #高频词字典，频次>2\n",
        "\n",
        "\n",
        "#从二维列表lst中过滤出高频词\n",
        "lst_high_freq = []\n",
        "for item in lst:\n",
        "  list_ = []\n",
        "  for i in item:\n",
        "    if i in dict_threshold.keys():\n",
        "      list_.append(i)\n",
        "    else:\n",
        "      continue\n",
        "  if list_:\n",
        "    lst_high_freq.append(list_)\n",
        "  else:\n",
        "    continue\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZSq_OfrvNG"
      },
      "source": [
        "lst_high_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOeYQjO3tG-6"
      },
      "source": [
        "u = (pd.get_dummies(pd.DataFrame(lst_high_freq), prefix='', prefix_sep='') #prefix 表示可以加的前缀。 lst 必须为二维列表，即列表内部元素为列表\n",
        "       .groupby(level=0, axis=1)\n",
        "       .sum())\n",
        "\n",
        "v = u.T.dot(u)\n",
        "v.values[(np.r_[:len(v)], ) * 2] = 0 #将自相关设为0，得短语共现矩阵\n",
        "\n",
        "#将共现矩阵中的词频转化为正向点对互信息（positive pointwise mutual information,PPMI）\n",
        "rows_name = v.index.tolist() #pd行名（高频词）列表,按顺序\n",
        "columns_name = v.columns.tolist()#pd列名（高频词）列表，按顺序\n",
        "\n",
        "for i in range(0,len(v)): #遍历v单元格值，将共现词频转化为PPMI\n",
        "    for j in range(0,len(v.columns)):\n",
        "#         print(pd_test.iloc[i][j])\n",
        "        if v.iloc[i][j] > 0:#单元格值大于0\n",
        "            b = dict_threshold.get(rows_name[i]) #从高频词字典中取出该高频词的频次\n",
        "            c = dict_threshold.get(columns_name[j]) #从高频词字典中取出该高频词的频次      \n",
        "            N = np.log2(v.iloc[i][j] * length / (b * c) * 1.0)  #length = len(lst)代表多少篇文档\n",
        "            # print(N)\n",
        "            if N < 0:\n",
        "                v.iloc[i][j] = 0\n",
        "            else:\n",
        "                v.iloc[i][j] = N\n",
        "        else: #单元格值等于0. PPMI=0\n",
        "                    continue           \n",
        "#更新v完毕\n",
        "# print(v) #type: pandas.dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAkx68MEGVkE"
      },
      "source": [
        "# 教程:pandas可以通过循环实时更新单元格的值\n",
        "# y= pd.DataFrame(np.arange(16).reshape(4,4))\n",
        "# llll = len(y)\n",
        "# for i in range(llll) :\n",
        "#   for j in range(llll):\n",
        "#     if y.iloc[i][j] > 8:\n",
        "#       y.iloc[i][j] = 0\n",
        "#     else:\n",
        "#       continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDE7E89YHv9z"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# print(\"get x and y pairs\")\n",
        "#get (x,y) pairs from the cooccurrence matrix\n",
        "arr = np.where(v>0) #PPMI大于0的元素的坐标\n",
        "corrs = [(v.index[x], v.columns[y]) for x, y in zip(*arr)] #所有满足共现次数（例如v>=1）的单元格的行列信息（例如[('A', 'B'), ('B', 'A'), ('B', 'D'), ('C', 'A'), ('D', 'B')]）。A-D均为行列名\n",
        "\n",
        "#get the unique pairs\n",
        "final_arr = [] #corrs中出现对称元素（例如('A', 'B'), ('B', 'A')），final_arr只存放非对称元素（例如只存储('A', 'B')，不存储 ('B', 'A')）\n",
        "\n",
        "for x, y in corrs:\n",
        "    if (y,x) not in final_arr:\n",
        "        final_arr.append((x,y))\n",
        " \n",
        "\n",
        "# 将 list of tuples （元组的列表）转化为Dataframe，并写入excel\n",
        "def convert_listToExcel_node(list_of_tuple): #将点数据转化为excel\n",
        "    # create DataFrame using data \n",
        "    df = pd.DataFrame(list_of_tuple, columns =['Id', 'Weight']) \n",
        "    df.insert(1,'Label',df['Id']) #在第二列插入第一列的值（复制第一列到第二列去：因为Gephi要求Label标签）\n",
        "    df.to_excel('/content/drive/My Drive/iConference/BIBM/nodes_2012_2019.xls') \n",
        "    ####################################################################################################\n",
        "\n",
        "def convert_listToExcel_edge(list_of_tuple): #将边数据转化为excel\n",
        "    # create DataFrame using data \n",
        "    df = pd.DataFrame(list_of_tuple, columns =['Source', 'Target', 'Weight']) \n",
        "    df['Type']='Undirected'\n",
        "    df.to_csv('/content/drive/My Drive/iConference/BIBM/edges_2012_2019.csv') # to_excel行超过65535报错，加上参数engine=’openpyxl’支持最大行数1048576\n",
        "    ####################################################################################################\n",
        "\n",
        "#get all the nodes and their frequency\n",
        "# nodes_list_with_duplicate = [i for i in item for item in lst] #将二维列表lst转化为一维\n",
        "# nodes_list = list(set(nodes_list_with_duplicate)) #点列表，已去重\n",
        "# nodes_list_freq = Counter(nodes_list) # （点列表,带频次）-----可导入 Gephi\n",
        "\n",
        "#get the weights of nodes\n",
        "weights = [] #关系列表（w为共现的次数）-----可导入 Gephi\n",
        "for x, y in final_arr:\n",
        "    w = v[y][x] #获取pandas中单元格的数据。即共现矩阵中词-词共现的次数（已转化为PPMI，存放于v）\n",
        "    weights.append((x,y,w))\n",
        "\n",
        "#construct the graph\n",
        "# G = nx.Graph()\n",
        "# nodes_vocabulary_list = nodes_list  #点列表(已去重)\n",
        "# G.add_nodes_from(nodes_vocabulary_list) #增加点\n",
        "# G.add_edges_from(final_arr) #增加边\n",
        "# G.add_weighted_edges_from(weights) #获得 边权重\n",
        "\n",
        "# Weights = nx.get_edge_attributes(G,'weight').values()\n",
        "# pos = nx.circular_layout(G) #固定graph的位置和形状\n",
        "# d = dict(G.degree) #取出节点的度数，转化为字典\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0jOXxOaHv91"
      },
      "source": [
        "#利用Network X 可视化Graph\n",
        "# nx.draw(G,\n",
        "#         pos,\n",
        "#         node_size = [i * 150 for i in d.values()], #节点大小,默认300.这里设置为与点度数成正比\n",
        "#         width=list(Weights), #权重\n",
        "#         with_labels=True, #是否带节点标签，默认True\n",
        "#         node_color='#A52A2A',#节点颜色\n",
        "#        edge_color ='#858585') #边颜色\n",
        "# # plt.savefig('ba.png') #显示方式1：将图片保存。图片质量？样式？不如用Gephi\n",
        "# # plt.show() #显示方式2：窗口显示"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMWfsIJHv92"
      },
      "source": [
        "#字典转化为list of tuple\n",
        "dict_threshold_avg = {k: v / len(lst) for k, v in dict_threshold.items()} #标准化：高频词出现次数/总篇数\n",
        "a_view =  dict_threshold_avg.items() #除以篇数后的字典\n",
        "a_list = list(a_view)\n",
        "\n",
        "convert_listToExcel_node(a_list) #输出点数据到excel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ljrC0tMBHv94"
      },
      "source": [
        "convert_listToExcel_edge(weights) #输出边数据到excel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPCDSO0MHl0n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCqMUYfzVzJZ"
      },
      "source": [
        "#Echarts示例\n",
        "!pip install pyecharts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2avg2ELHVy_Z",
        "outputId": "fb326c9c-71e8-4d82-cb2b-2634b64062b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "#Echarts 绘制关系图,未解决？？？？\n",
        "import json\n",
        "from pyecharts import options as opts\n",
        "from pyecharts.charts import Graph\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/iConference/AAAI_1981_1987.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    j = json.load(f)\n",
        "    nodes = j[\"nodes\"]\n",
        "    links = j[\"deges\"]\n",
        "    categories = j[\"categories\"]\n",
        "\n",
        "c = (\n",
        "    Graph(init_opts=opts.InitOpts(width=\"1000px\", height=\"600px\"))\n",
        "    .add(\n",
        "        \"\",\n",
        "        nodes=nodes,\n",
        "        links=links,\n",
        "        categories=categories,\n",
        "        layout=\"circular\",\n",
        "        is_rotate_label=True,\n",
        "        linestyle_opts=opts.LineStyleOpts(color=\"source\", curve=0.3),\n",
        "        label_opts=opts.LabelOpts(position=\"right\"),\n",
        "    )\n",
        "    .set_global_opts(\n",
        "        title_opts=opts.TitleOpts(title=\"Graph-Les Miserables\"),\n",
        "        legend_opts=opts.LegendOpts(orient=\"vertical\", pos_left=\"2%\", pos_top=\"20%\"),\n",
        "    )\n",
        "    .render(\"graph_les_miserables.html\")\n",
        ")\n",
        "#   return c\n",
        "\n",
        "# # 需要安装 snapshot-selenium 或者 snapshot-phantomjs\n",
        "# make_snapshot(driver, bar_chart().render(), \"bar.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-da33be2f67c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/iConference/AAAI_1981_1987.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nodes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deges\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'nodes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVwZQYvvWDWJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYkKPDihHmLs"
      },
      "source": [
        "#\n",
        "pd_test=pd.DataFrame({\"A\":[1,2,3,4],\"B\":[5,6,7,8],\"C\":[1,1,1,1]})\n",
        "for index,row in df.iterrows():\n",
        "\tif row['A']>1:\n",
        "\t\trow['B']=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFeqe9PNHmgD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "174qj2bcHv95"
      },
      "source": [
        "#Networkx 用法示例\n",
        "G = nx.Graph()\n",
        "G.add_edge(0,1,color='r',weight=2)\n",
        "G.add_edge(1,2,color='g',weight=4)\n",
        "G.add_edge(2,3,color='b',weight=16)\n",
        "G.add_edge(3,4,color='y',weight=3)\n",
        "G.add_edge(4,0,color='m',weight=1)\n",
        "G.add_edge(1,3,color='#0CC',weight=2)\n",
        "\n",
        "colors = nx.get_edge_attributes(G,'color').values()\n",
        "weights = nx.get_edge_attributes(G,'weight').values()\n",
        "\n",
        "pos = nx.circular_layout(G)\n",
        "nx.draw(G, pos, \n",
        "        edge_color=colors, \n",
        "        width=list(weights),\n",
        "        with_labels=True,\n",
        "        node_color='lightgreen')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwUHMsbDHv98",
        "outputId": "c120e7e4-2a21-4cb3-8305-c37b9ce086df"
      },
      "source": [
        "#get_dummies用法\n",
        "df1 = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'], 'C': [1, 2, 3]})\n",
        "pd.get_dummies(df1, prefix=['col1', 'col2'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>C</th>\n",
              "      <th>col1_a</th>\n",
              "      <th>col1_b</th>\n",
              "      <th>col2_a</th>\n",
              "      <th>col2_b</th>\n",
              "      <th>col2_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   C  col1_a  col1_b  col2_a  col2_b  col2_c\n",
              "0  1       1       0       0       1       0\n",
              "1  2       0       1       1       0       0\n",
              "2  3       1       0       0       0       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K_GtpXiHv9-"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U3-Jyy2Hv9_",
        "outputId": "5680e8dc-354a-48d1-f951-a29af99638a3"
      },
      "source": [
        "lst = [\"['-NER', 'ML8']\",\"['ML', 'DL', 'NB']\"]\n",
        "\n",
        "def Convert_(list): #将列表内元素string转化为list\n",
        "    d2_list = []\n",
        "    for k in list:\n",
        "        result_list = re.findall('[a-zA-Z0-9]+',k) #提取英文字母或数字\n",
        "        d2_list.append(result_list)\n",
        "    return d2_list\n",
        "\n",
        "Convert_(lst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['NER', 'ML8'], ['ML', 'DL', 'NB']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rfZyef0Hv-B"
      },
      "source": [
        "#按照字典的健值排序示例\n",
        "def Counter(list):\n",
        "    dict={} #定义字典存放词及其频次\n",
        "    for i in list:\n",
        "        if i in dict.keys():\n",
        "            dict[i]+=1\n",
        "        else:\n",
        "            dict[i]=1\n",
        "    dict_sorted = {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])} #按照频次降序排列 \n",
        "    return dict_sorted\n",
        "\n",
        "\n",
        "# def sortedDictValues3(**adict): #按照字典的健值排序\n",
        "#     keys =  adict.keys()\n",
        "#     keys.sort()\n",
        "#     return map(adict.get, keys)\n",
        "\n",
        "# [(k,di[k]) for k in sorted(di.keys())] #按照字典的健值排序\n",
        "\n",
        "ddd = ['a','a','a','b','b','c','c','c','c']\n",
        "Counter(ddd) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKHFK9ZuHv-C"
      },
      "source": [
        "#提取（过滤/筛选）含有目标关键词的行，存入 excel\n",
        "df_AI_Topicwords = pd.read_excel('/content/drive/My Drive/iConference/BIBM/AI_topic_keywords.xlsx',sheet_name='Sheet1',header=0) #header =0 默认第行为列标题\n",
        "\n",
        "df_AI_Topicwords ['keywords'] = df_AI_Topicwords ['keywords'].str.strip() #对目标关键字，剔除左右的空格\n",
        "\n",
        "AI_TOPIC_KEYWORDS = df_AI_Topicwords ['keywords'].values.tolist()#将其转化为list\n",
        "AI_topic_keywords = [x.lower() for x in AI_TOPIC_KEYWORDS] #全部小写df['noun_phrases']\n",
        "\n",
        "df_target_papers = df[df['title_abstract'].str.contains('|'.join(AI_topic_keywords),na=False)] #检查行是否包含目标关键词\n",
        "\n",
        "df_target_papers.to_excel(r'/content/drive/My Drive/iConference/BIBM/AI_TARGET_TOTAL_COPY.xlsx', index = False) #输出\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcf1uLrzSvry"
      },
      "source": [
        "df_AI_Topicwords.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}